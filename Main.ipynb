{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import Mapping\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data1.4/'\n",
    "save_path='save/'\n",
    "\n",
    "bsz=128\n",
    "cuda=True\n",
    "device=3\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "num_gene_compressed_drug=64\n",
    "num_gene_compressed_cell=128\n",
    "\n",
    "#isClassification=True #False for regression task\n",
    "syn_threshold=30\n",
    "ri_threshold=50\n",
    "\n",
    "log_interval=100\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drug pair, cell, scoers\n",
    "df=pickle.load(open(data_path+'summary_mean.p', 'rb'))\n",
    "codes=pickle.load(open(data_path+'codes.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug_row</th>\n",
       "      <th>drug_col</th>\n",
       "      <th>cell_line_name</th>\n",
       "      <th>study_id</th>\n",
       "      <th>ri_row</th>\n",
       "      <th>ri_col</th>\n",
       "      <th>synergy_loewe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433</td>\n",
       "      <td>4127</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-21.0794</td>\n",
       "      <td>17.392589</td>\n",
       "      <td>4.436431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>2848</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>-20.0430</td>\n",
       "      <td>25.595000</td>\n",
       "      <td>-44.555935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>2720</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>-9.7760</td>\n",
       "      <td>29.111000</td>\n",
       "      <td>-37.189720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2848</td>\n",
       "      <td>2848</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6.7970</td>\n",
       "      <td>6.964000</td>\n",
       "      <td>1.283298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2848</td>\n",
       "      <td>2848</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>11.5280</td>\n",
       "      <td>7.190000</td>\n",
       "      <td>-3.028745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   drug_row  drug_col  cell_line_name  study_id   ri_row     ri_col  \\\n",
       "0      1433      4127               0         4 -21.0794  17.392589   \n",
       "1      1998      2848               1         6 -20.0430  25.595000   \n",
       "2      1998      2720               1         6  -9.7760  29.111000   \n",
       "3      2848      2848               2        12   6.7970   6.964000   \n",
       "4      2848      2848               3        12  11.5280   7.190000   \n",
       "\n",
       "   synergy_loewe  \n",
       "0       4.436431  \n",
       "1     -44.555935  \n",
       "2     -37.189720  \n",
       "3       1.283298  \n",
       "4      -3.028745  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug's external features\n",
    "drug_features=pickle.load(open(data_path+'drug_features.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell's external features\n",
    "cell_features=pickle.load(open(data_path+'cell_features.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_celllines= len(codes['cell'].idx2item)\n",
    "num_drugs=len(codes['drugs'].idx2item)\n",
    "\n",
    "num_genes = len(codes['gene'].idx2item)\n",
    "num_tissue = len(codes['tissue'].idx2item)\n",
    "num_disease = len(codes['disease'].idx2item)\n",
    "\n",
    "num_drug_fp=len(drug_features.loc[0,'fps'])\n",
    "max_drug_sm_len = drug_features['smiles'].apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugTargetDataset(Dataset):\n",
    "    def __init__(self, drug_features):\n",
    "        self.drug_features = drug_features\n",
    "    def __len__(self):\n",
    "        return len(self.drug_features)\n",
    "    def __getitem__(self,idx):\n",
    "        gene_ids=self.drug_features.loc[idx, 'gene_id']\n",
    "        genes=np.zeros(num_genes)\n",
    "        genes[gene_ids]=1\n",
    "        \n",
    "        return genes\n",
    "    \n",
    "class CellGeneDataset(Dataset):\n",
    "    def __init__(self, cell_features):\n",
    "        self.cell_features = cell_features\n",
    "    def __len__(self):\n",
    "        return len(self.cell_features)\n",
    "    def __getitem__(self,idx):\n",
    "        gene_ids=self.cell_features.loc[idx,'gene_id']\n",
    "        genes = np.zeros(num_genes)\n",
    "        for key,value in gene_ids.items():\n",
    "            genes[key]=value\n",
    "            \n",
    "        return genes\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two layers of fully connected layers\n",
    "class FC2(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(FC2, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(in_features)\n",
    "        self.fc1 = nn.Linear(in_features, int(in_features/2))\n",
    "        self.fc2 = nn.Linear(int(in_features/2),out_features)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress gene features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneCompressor(nn.Module):\n",
    "    def __init__(self, num_in, num_out, dropout=0.1):\n",
    "        super(GeneCompressor, self).__init__()\n",
    "        self.dropout=dropout\n",
    "        self.encoder=nn.Linear(num_in, num_out)\n",
    "        self.decoder=nn.Linear(num_out,num_in)\n",
    "\n",
    "    def _encoder(self,x):\n",
    "        return F.dropout(F.relu(self.encoder(x)), self.dropout, training=self.training)\n",
    "    \n",
    "    def _decoder(self,x):\n",
    "        return F.dropout(self.decoder(x), self.dropout, training=self.training)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self._encoder(x)\n",
    "        x=self._decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneCompressing(data_loader,num_gene_compressed, noise_weight=0.2, epochs=20, log_interval=10 ):\n",
    "    #model\n",
    "    geneCompressor=GeneCompressor(num_genes, num_out=num_gene_compressed, dropout=0.1)\n",
    "    if cuda: \n",
    "        geneCompressor=geneCompressor.cuda()\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=optim.Adam(geneCompressor.parameters())\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #train\n",
    "        geneCompressor.train()\n",
    "        total_loss=0\n",
    "        start_time=time.time()\n",
    "        for iteration, gene in enumerate(data_loader):\n",
    "            gene=Variable(gene).float()\n",
    "            noise=noise_weight*torch.randn(gene.shape)\n",
    "\n",
    "            if cuda:\n",
    "                gene=gene.cuda()\n",
    "                noise=noise.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output=geneCompressor(gene+noise)\n",
    "            loss=criterion(output,gene)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            if iteration % log_interval == 0 and iteration > 0:\n",
    "                cur_loss = total_loss.item() / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:8.5f}'.format(epoch, iteration, int(len(data_loader)/bsz), elapsed * 1000/log_interval, cur_loss))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "#         #test\n",
    "#         geneCompressor.eval()\n",
    "#         total_loss=0\n",
    "#         start_time=time.time()\n",
    "#         with torch.no_grad():\n",
    "#             for iteration, gene in enumerate(test_data_loader):\n",
    "#                 gene=Variable(gene).float()\n",
    "#                 if cuda:\n",
    "#                     gene=gene.cuda(device)\n",
    "#                 output=geneCompressor(gene)\n",
    "#                 loss=criterion(output,gene)\n",
    "#                 total_loss += loss.data\n",
    "#             print(total_loss.item()/iteration)\n",
    "    return geneCompressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drug's target gene data\n",
    "drugGeneDataset=DrugTargetDataset(drug_features)\n",
    "drugGeneDataset_loader = DataLoader(drugGeneDataset, batch_size=64, shuffle=True)\n",
    "#learn\n",
    "drugGeneCompressor=geneCompressing(drugGeneDataset_loader, num_gene_compressed_drug)\n",
    "#save\n",
    "drugGeneCompressor.eval()\n",
    "drugGeneCompressed=np.array([drugGeneCompressor.cpu()._encoder(torch.FloatTensor(drugGeneDataset[d])).data.numpy() for d in range(num_drugs)])\n",
    "torch.save(drugGeneCompressor.state_dict(), data_path+'drugGeneCompressor.p')\n",
    "pickle.dump(drugGeneCompressed, open(data_path+'drugGeneCompressed.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugGeneCompressed=pickle.load(open(data_path+'drugGeneCompressed.p', 'rb'))\n",
    "drugGeneCompressed=torch.FloatTensor(drugGeneCompressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cell line's gene expression data\n",
    "cellGeneDataset=CellGeneDataset(cell_features)\n",
    "cellGeneDataset_loader = DataLoader(cellGeneDataset, batch_size=64, shuffle=True)\n",
    "#learn\n",
    "cellGeneCompressor=geneCompressing(cellGeneDataset_loader,num_gene_compressed_cell, noise_weight=0.01, log_interval=1 )\n",
    "#save\n",
    "cellGeneCompressor.eval()\n",
    "cellGeneCompressed=np.array([cellGeneCompressor.cpu()._encoder(torch.FloatTensor(cellGeneDataset[d])).data.numpy() for d in range(num_celllines)])\n",
    "torch.save(cellGeneCompressor.state_dict(), data_path+'cellGeneCompressor.p')\n",
    "pickle.dump(cellGeneCompressed, open(data_path+'cellGeneCompressed.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellGeneCompressed=pickle.load(open(data_path+'cellGeneCompressed.p', 'rb'))\n",
    "cellGeneCompressed=torch.FloatTensor(cellGeneCompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split in cross or external validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_of_interest(tissues):\n",
    "    tissues_of_interests = [codes['tissue'].item2idx[minor_tissue] for minor_tissue in tissues]\n",
    "    cell_of_interest = cell_features.index[cell_features['tissue_id'].isin(tissues_of_interests)].tolist()\n",
    "    return cell_of_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test for general model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_tissues=['bone', 'prostate' ]\n",
    "cell_of_interest = get_cell_of_interest(minor_tissues)\n",
    "df_tissue_of_interest = df.loc[df['cell_line_name'].isin(cell_of_interest),:]\n",
    "df_all = df.drop(df_tissue_of_interest.index)\n",
    "#specific database\n",
    "#df_all=df_all.loc[df_all['study_id']==3]\n",
    "#cross validation\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.2) #cross validation\n",
    "#external validation\n",
    "#df_train=df_all.loc[df_all['study_id']==3] 3: 'ALMANAC'\n",
    "#df_test=df_all.loc[df_all['study_id']==1] 1: 'ONEIL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test for bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_bone = df.loc[df['cell_line_name'].isin(get_cell_of_interest(['bone'])),:]\n",
    "# Cross validation\n",
    "_df_train_bone, _df_test_bone = train_test_split(_df_bone, test_size=0.2, random_state=1)\n",
    "# External validation\n",
    "#_df_train_bone=_df_bone.loc[_df_bone['study_id']!=9]\n",
    "#_df_test_bone= _df_bone.loc[_df_bone['study_id']==9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test for prostate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_prostate= df.loc[df['cell_line_name'].isin(get_cell_of_interest(['prostate'])),:]\n",
    "#Cross validation\n",
    "_df_train_prostate, _df_test_prostate = train_test_split(_df_prostate, test_size=0.2, random_state=1)\n",
    "# External validation\n",
    "#_df_train_prostate=_df_prostate.loc[_df_prostate['study_id']!=1]\n",
    "#_df_test_prostate=_df_prostate.loc[_df_prostate['study_id']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugCombDataset(Dataset):\n",
    "    def __init__(self, df, drug_features, cell_features):\n",
    "        self.df = df\n",
    "        self.drug_features = drug_features\n",
    "        self.cell_features = cell_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        d1 = self.df.iloc[idx, 0]\n",
    "        d2 = self.df.iloc[idx, 1]\n",
    "        cell = self.df.iloc[idx,2]\n",
    "        ri_d1 = 1.0 if self.df.iloc[idx,3] >ri_threshold else 0\n",
    "        ri_d2 = 1.0 if self.df.iloc[idx,4] >ri_threshold else 0\n",
    "        syn = 1.0 if self.df.iloc[idx, 5] >syn_threshold else 0\n",
    "        \n",
    "        \n",
    "        #external features\n",
    "        d1_fp = np.array(self.drug_features.loc[d1, 'fps'])\n",
    "        d1_sm = self.drug_features.loc[d1, 'smiles']\n",
    "        d1_sm = np.pad(d1_sm, pad_width=(0, max_drug_sm_len-len(d1_sm)), mode='constant', constant_values=0)\n",
    "        d1_gn=drugGeneCompressed[d1]\n",
    "        \n",
    "        d2_fp = np.array(self.drug_features.loc[d2, 'fps'])\n",
    "        d2_sm = self.drug_features.loc[d2, 'smiles']\n",
    "        d2_sm = np.pad(d2_sm, pad_width=(0, max_drug_sm_len-len(d2_sm)), mode='constant', constant_values=0)\n",
    "        d2_gn=drugGeneCompressed[d2]\n",
    "        \n",
    "        c_ts = self.cell_features.loc[cell, 'tissue_id']\n",
    "        c_ds = self.cell_features.loc[cell, 'disease_id']\n",
    "        c_gn= cellGeneCompressed[cell]\n",
    "        \n",
    "        sample = {\n",
    "            'd1': d1,\n",
    "            'd1_fp': d1_fp,\n",
    "            'd1_sm': d1_sm,\n",
    "            'd1_gn': d1_gn,\n",
    "            \n",
    "            'd2': d2,\n",
    "            'd2_fp': d2_fp,\n",
    "            'd2_sm': d2_sm,\n",
    "            'd2_gn': d2_gn,\n",
    "            \n",
    "            'cell': cell,\n",
    "            'c_ts': c_ts,\n",
    "            'c_ds': c_ds, #missing -1\n",
    "            'c_gn': c_gn,\n",
    "            \n",
    "            'ri_d1': ri_d1,\n",
    "            'ri_d2': ri_d2,\n",
    "            'syn': syn\n",
    "        }\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DrugCombDataset(df_train, drug_features, cell_features)\n",
    "train_loader = DataLoader(train, batch_size=bsz, shuffle=True )\n",
    "test = DrugCombDataset(df_test, drug_features, cell_features)\n",
    "test_loader = DataLoader(test, batch_size=bsz, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_bone = DrugCombDataset(_df_train_bone, drug_features, cell_features)\n",
    "_train_loader_bone = DataLoader(_train_bone, batch_size=bsz, shuffle=True )\n",
    "_test_bone = DrugCombDataset(_df_test_bone, drug_features, cell_features)\n",
    "_test_loader_bone = DataLoader(_test_bone, batch_size=bsz, shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prostate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_prostate = DrugCombDataset(_df_train_prostate, drug_features, cell_features)\n",
    "_train_loader_prostate = DataLoader(_train_prostate, batch_size=bsz, shuffle=True )\n",
    "_test_prostate = DrugCombDataset(_df_test_prostate, drug_features, cell_features)\n",
    "_test_loader_prostate = DataLoader(_test_prostate, batch_size=bsz, shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_drugs=num_drugs,\n",
    "                 num_ID_emb=0,\n",
    "                 num_drug_fp=num_drug_fp,\n",
    "                 max_drug_sm_len=max_drug_sm_len,\n",
    "                 num_gene = num_gene_compressed_drug,\n",
    "                 num_comp_char=len(codes['mole'].idx2item),\n",
    "                 fp_embed_sz = 32,\n",
    "                 gene_embed_sz = int(num_gene_compressed_drug/2),\n",
    "                 out_size=64,\n",
    "                 dropout=0.3):\n",
    "        super(DrugEncoder, self).__init__()\n",
    "        \n",
    "        self.dropout= dropout\n",
    "        #DRUG\n",
    "        #drug ID\n",
    "        #self.embed_id = nn.Embedding(num_drugs, num_ID_emb)\n",
    "        \n",
    "        #compound ID\n",
    "        self.embed_comp = nn.Embedding(num_comp_char, num_comp_char, padding_idx=0)#padding's idx=0\n",
    "        #encoding compound\n",
    "        self.encoderlayer = nn.TransformerEncoderLayer(d_model=num_comp_char, nhead=4)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderlayer, num_layers=1)\n",
    "        \n",
    "        #fingerprint\n",
    "        self.dense_fp = nn.Linear(num_drug_fp,fp_embed_sz)\n",
    "        #gene\n",
    "        self.dense_gene = nn.Linear(num_gene,gene_embed_sz)\n",
    "        \n",
    "        #depthwise for compound encoding\n",
    "        self.conv = nn.Conv2d(1, 1, (1, num_comp_char), groups=1)\n",
    "        \n",
    "        #combined\n",
    "        combined_sz = num_ID_emb+fp_embed_sz+max_drug_sm_len+gene_embed_sz\n",
    "        self.FC2 = FC2(combined_sz, out_size, dropout)\n",
    "\n",
    "    def forward(self, d_list):\n",
    "        \"\"\"\n",
    "            id: bsz*1\n",
    "            fp: bsz*num_drug_fp\n",
    "            sm: bsz*max_drug_sm_len\n",
    "        \"\"\"\n",
    "        id, fp, sm, gn = d_list\n",
    "        \n",
    "        sm = self.embed_comp(sm) #bsz*max_drug_sm_len*num_comp_char(embedding size)\n",
    "        sm = self.encoder(sm)\n",
    "        sm = self.conv(sm.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        fp = F.relu(self.dense_fp(fp))\n",
    "        gn = F.relu(self.dense_gene(gn))\n",
    "        \n",
    "        #combine\n",
    "        x = torch.cat((fp, sm, gn),1) # bsz*[num_emb_id+num_drug_fp+max+drug_sm]\n",
    "        x = self.FC2(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_cells=num_celllines,\n",
    "                 num_tissue=0,\n",
    "                 num_disease=num_disease,\n",
    "                 num_ID_emb=0,\n",
    "                 gene_embed_sz=int(num_gene_compressed_cell/2),\n",
    "                 num_gene=num_gene_compressed_cell,\n",
    "                 out_size=64,\n",
    "                 dropout=0.3):\n",
    "        super(CellEncoder, self).__init__()\n",
    "        \n",
    "        self.dropout= dropout\n",
    "        #cell ID\n",
    "        #self.embed_id = nn.Embedding(num_cells, num_ID_emb)\n",
    "        #cell tissue\n",
    "        #self.embed_ts = nn.Embedding(num_tissue, num_tissue)\n",
    "        #cell disease\n",
    "        self.embed_ds = nn.Embedding(num_disease, num_disease, padding_idx=3)\n",
    "        #gene\n",
    "        self.dense_gene = nn.Linear(num_gene,gene_embed_sz)\n",
    "        \n",
    "        #combined\n",
    "        combined_sz = num_ID_emb+num_tissue+num_disease+gene_embed_sz\n",
    "        self.FC2 = FC2(combined_sz, out_size, dropout)\n",
    "    \n",
    "        \n",
    "    def forward(self, c_list):\n",
    "        \"\"\"\n",
    "            id: bsz*1\n",
    "            fp: bsz*num_drug_fp\n",
    "            sm: bsz*max_drug_sm_len\n",
    "        \"\"\"\n",
    "        id, ts, ds, gn = c_list\n",
    "        ds = F.relu(self.embed_ds(ds)) #bsz*num_diesaes\n",
    "        \n",
    "        gn = F.relu(self.dense_gene(gn)) #bsz*gene_embed_sz\n",
    "        \n",
    "        #combine\n",
    "        x = torch.cat((ds, gn),1) # bsz*combined_sz\n",
    "        x = self.FC2(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comb(nn.Module):\n",
    "    def __init__(self, num_cells=num_celllines, \n",
    "                 num_drugs=num_drugs,\n",
    "                 num_drug_fp=num_drug_fp,\n",
    "                 max_drug_sm_len=max_drug_sm_len,\n",
    "                num_comp_char=len(codes['mole'].idx2item),\n",
    "                 num_ID_emb=0,\n",
    "                 out_size=64,\n",
    "                dropout=0.3):\n",
    "        \n",
    "        super(Comb, self).__init__()\n",
    "        \n",
    "        self.dropout=dropout    \n",
    "        #drug\n",
    "        self.drugEncoder = DrugEncoder()\n",
    "        #cell\n",
    "        self.cellEncoder = CellEncoder()\n",
    "        #fc\n",
    "        self.fc_syn = FC2(out_size*3, 1, dropout)\n",
    "        self.fc_ri = FC2(out_size*2, 1, dropout)\n",
    "        \n",
    "    def forward(self, d1_list, d2_list, c_list):\n",
    "        d1 = self.drugEncoder(d1_list)\n",
    "        d2 = self.drugEncoder(d2_list)\n",
    "        c = self.cellEncoder(c_list)\n",
    "        \n",
    "        syn = self.fc_syn(torch.cat((d1, d2, c),1))\n",
    "        ri1 = self.fc_ri(torch.cat((d1,c),1))\n",
    "        ri2 = self.fc_ri(torch.cat((d2,c),1))\n",
    "        \n",
    "        return syn, ri1, ri2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Comb()\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "#Regression\n",
    "#criterion_mse = nn.MSELoss()\n",
    "#Classification\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adagrad(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def training(isAux, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for iteration, sample in enumerate(data_loader):\n",
    "        d1=Variable(sample['d1'])\n",
    "        d1_fp = Variable(sample['d1_fp'].float())\n",
    "        d1_sm = Variable(sample['d1_sm'])\n",
    "        d1_gn = Variable(sample['d1_gn'].float())\n",
    "        \n",
    "        d2=Variable(sample['d2'])\n",
    "        d2_fp = Variable(sample['d2_fp'].float())\n",
    "        d2_sm = Variable(sample['d2_sm'])\n",
    "        d2_gn = Variable(sample['d2_gn'].float())\n",
    "        \n",
    "        cell = Variable(sample['cell'])\n",
    "        c_ts = Variable(sample['c_ts'])\n",
    "        c_ds = Variable(sample['c_ds'])\n",
    "        c_gn = Variable(sample['c_gn'].float())\n",
    "        \n",
    "        syn_true = Variable(sample['syn'].float())\n",
    "        ri_d1=Variable(sample['ri_d1'].float())\n",
    "        ri_d2=Variable(sample['ri_d2'].float())\n",
    "\n",
    "\n",
    "        if cuda:\n",
    "            d1=d1.cuda()\n",
    "            d1_fp=d1_fp.cuda()\n",
    "            d1_sm=d1_sm.cuda()\n",
    "            d1_gn=d1_gn.cuda()\n",
    "            \n",
    "            d2=d2.cuda()\n",
    "            d2_fp=d2_fp.cuda()\n",
    "            d2_sm=d2_sm.cuda()\n",
    "            d2_gn=d2_gn.cuda()\n",
    "            \n",
    "            cell=cell.cuda()\n",
    "            c_ts=c_ts.cuda()\n",
    "            c_ds=c_ds.cuda()\n",
    "            c_gn=c_gn.cuda()\n",
    "            \n",
    "            syn_true=syn_true.cuda()\n",
    "            ri_d1=ri_d1.cuda()\n",
    "            ri_d2=ri_d2.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        syn, ri1, ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts, c_ds, c_gn) )\n",
    "        \n",
    "        if not isAux:\n",
    "            loss = criterion_bce(syn, syn_true.view(-1,1))\n",
    "        else:\n",
    "            loss = criterion_bce(ri1, ri_d1.view(-1,1))+criterion_bce(ri2, ri_d2.view(-1,1))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if iteration % log_interval == 0 and iteration > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:8.5f}'.format(epoch, iteration, int(len(train_loader)/bsz), elapsed * 1000/log_interval, cur_loss))\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_sen = 0\n",
    "\n",
    "    \n",
    "    #loss\n",
    "    with torch.no_grad():\n",
    "        for iteration, sample in enumerate(data_loader):\n",
    "            d1=Variable(sample['d1'])\n",
    "            d1_fp = Variable(sample['d1_fp'].float())\n",
    "            d1_sm = Variable(sample['d1_sm'])\n",
    "            d1_gn = Variable(sample['d1_gn'].float())\n",
    "\n",
    "            d2=Variable(sample['d2'])\n",
    "            d2_fp = Variable(sample['d2_fp'].float())\n",
    "            d2_sm = Variable(sample['d2_sm'])\n",
    "            d2_gn = Variable(sample['d2_gn'].float())\n",
    "\n",
    "            cell = Variable(sample['cell'])\n",
    "            c_ts = Variable(sample['c_ts'])\n",
    "            c_ds = Variable(sample['c_ds'])\n",
    "            c_gn = Variable(sample['c_gn'].float())\n",
    "\n",
    "            syn_true = Variable(sample['syn'].float())\n",
    "            ri_d1=Variable(sample['ri_d1'].float())\n",
    "            ri_d2=Variable(sample['ri_d2'].float())\n",
    "\n",
    "\n",
    "            if cuda:\n",
    "                d1=d1.cuda()\n",
    "                d1_fp=d1_fp.cuda()\n",
    "                d1_sm=d1_sm.cuda()\n",
    "                d1_gn=d1_gn.cuda()\n",
    "\n",
    "                d2=d2.cuda()\n",
    "                d2_fp=d2_fp.cuda()\n",
    "                d2_sm=d2_sm.cuda()\n",
    "                d2_gn=d2_gn.cuda()\n",
    "\n",
    "                cell=cell.cuda()\n",
    "                c_ts=c_ts.cuda()\n",
    "                c_ds=c_ds.cuda()\n",
    "                c_gn=c_gn.cuda()\n",
    "\n",
    "                syn_true=syn_true.cuda()\n",
    "                ri_d1=ri_d1.cuda()\n",
    "                ri_d2=ri_d2.cuda()\n",
    "\n",
    "\n",
    "            syn,ri1,ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts,c_ds,c_gn) )\n",
    "            loss = criterion_bce(syn, syn_true.view(-1,1))\n",
    "            total_loss +=loss.data\n",
    "            loss_sen = (criterion_bce(ri1, ri_d1.view(-1,1))+criterion_bce(ri2, ri_d2.view(-1,1)))/2\n",
    "            total_loss_sen += loss_sen.data\n",
    "\n",
    "        print('syn mse', total_loss.item()/(iteration+1))\n",
    "        print('sen_mse', total_loss_sen.item()/(iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/   14 batches | ms/batch 90.74 | loss  0.31913\n",
      "| epoch   1 |   200/   14 batches | ms/batch 87.11 | loss  0.25310\n",
      "| epoch   1 |   300/   14 batches | ms/batch 85.94 | loss  0.24393\n",
      "| epoch   1 |   400/   14 batches | ms/batch 85.92 | loss  0.24219\n",
      "| epoch   1 |   500/   14 batches | ms/batch 86.10 | loss  0.23739\n",
      "| epoch   1 |   600/   14 batches | ms/batch 86.01 | loss  0.23300\n",
      "| epoch   1 |   700/   14 batches | ms/batch 85.91 | loss  0.23055\n",
      "| epoch   1 |   800/   14 batches | ms/batch 87.91 | loss  0.23335\n",
      "| epoch   1 |   900/   14 batches | ms/batch 90.00 | loss  0.24583\n",
      "| epoch   1 |  1000/   14 batches | ms/batch 89.77 | loss  0.22782\n",
      "| epoch   1 |  1100/   14 batches | ms/batch 90.07 | loss  0.23303\n",
      "| epoch   1 |  1200/   14 batches | ms/batch 89.87 | loss  0.23319\n",
      "| epoch   1 |  1300/   14 batches | ms/batch 89.22 | loss  0.22923\n",
      "| epoch   1 |  1400/   14 batches | ms/batch 85.74 | loss  0.22204\n",
      "| epoch   1 |  1500/   14 batches | ms/batch 86.54 | loss  0.23013\n",
      "| epoch   1 |  1600/   14 batches | ms/batch 85.95 | loss  0.23676\n",
      "| epoch   1 |  1700/   14 batches | ms/batch 85.91 | loss  0.22307\n",
      "| epoch   1 |  1800/   14 batches | ms/batch 85.81 | loss  0.22653\n",
      "| epoch   1 |   100/   14 batches | ms/batch 87.15 | loss  0.19959\n",
      "| epoch   1 |   200/   14 batches | ms/batch 86.93 | loss  0.12700\n",
      "| epoch   1 |   300/   14 batches | ms/batch 87.55 | loss  0.12009\n",
      "| epoch   1 |   400/   14 batches | ms/batch 86.27 | loss  0.11233\n",
      "| epoch   1 |   500/   14 batches | ms/batch 85.98 | loss  0.11484\n",
      "| epoch   1 |   600/   14 batches | ms/batch 86.28 | loss  0.12155\n",
      "| epoch   1 |   700/   14 batches | ms/batch 86.27 | loss  0.10651\n",
      "| epoch   1 |   800/   14 batches | ms/batch 86.11 | loss  0.10925\n",
      "| epoch   1 |   900/   14 batches | ms/batch 86.12 | loss  0.11516\n",
      "| epoch   1 |  1000/   14 batches | ms/batch 86.44 | loss  0.12282\n",
      "| epoch   1 |  1100/   14 batches | ms/batch 86.23 | loss  0.11197\n",
      "| epoch   1 |  1200/   14 batches | ms/batch 86.07 | loss  0.10405\n",
      "| epoch   1 |  1300/   14 batches | ms/batch 89.98 | loss  0.11201\n",
      "| epoch   1 |  1400/   14 batches | ms/batch 90.20 | loss  0.10633\n",
      "| epoch   1 |  1500/   14 batches | ms/batch 90.40 | loss  0.10212\n",
      "| epoch   1 |  1600/   14 batches | ms/batch 87.76 | loss  0.11613\n",
      "| epoch   1 |  1700/   14 batches | ms/batch 86.91 | loss  0.10469\n",
      "| epoch   1 |  1800/   14 batches | ms/batch 86.01 | loss  0.11678\n",
      "syn mse 0.22499095884143797\n",
      "sen_mse 0.05362824089506753\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/   14 batches | ms/batch 87.18 | loss  0.23911\n",
      "| epoch   2 |   200/   14 batches | ms/batch 86.15 | loss  0.22450\n",
      "| epoch   2 |   300/   14 batches | ms/batch 87.15 | loss  0.22856\n",
      "| epoch   2 |   400/   14 batches | ms/batch 87.61 | loss  0.22387\n",
      "| epoch   2 |   500/   14 batches | ms/batch 87.58 | loss  0.23173\n",
      "| epoch   2 |   600/   14 batches | ms/batch 87.26 | loss  0.22326\n",
      "| epoch   2 |   700/   14 batches | ms/batch 86.68 | loss  0.21946\n",
      "| epoch   2 |   800/   14 batches | ms/batch 87.39 | loss  0.21617\n",
      "| epoch   2 |   900/   14 batches | ms/batch 87.37 | loss  0.22881\n",
      "| epoch   2 |  1000/   14 batches | ms/batch 87.03 | loss  0.21807\n",
      "| epoch   2 |  1100/   14 batches | ms/batch 87.38 | loss  0.21757\n",
      "| epoch   2 |  1200/   14 batches | ms/batch 86.28 | loss  0.22840\n",
      "| epoch   2 |  1300/   14 batches | ms/batch 86.45 | loss  0.21866\n",
      "| epoch   2 |  1400/   14 batches | ms/batch 86.08 | loss  0.21073\n",
      "| epoch   2 |  1500/   14 batches | ms/batch 86.22 | loss  0.21802\n",
      "| epoch   2 |  1600/   14 batches | ms/batch 86.29 | loss  0.20990\n",
      "| epoch   2 |  1700/   14 batches | ms/batch 86.33 | loss  0.21539\n",
      "| epoch   2 |  1800/   14 batches | ms/batch 85.96 | loss  0.21662\n",
      "| epoch   2 |   100/   14 batches | ms/batch 87.28 | loss  0.12096\n",
      "| epoch   2 |   200/   14 batches | ms/batch 86.17 | loss  0.11551\n",
      "| epoch   2 |   300/   14 batches | ms/batch 86.25 | loss  0.10206\n",
      "| epoch   2 |   400/   14 batches | ms/batch 86.18 | loss  0.10907\n",
      "| epoch   2 |   500/   14 batches | ms/batch 87.05 | loss  0.12362\n",
      "| epoch   2 |   600/   14 batches | ms/batch 86.49 | loss  0.10150\n",
      "| epoch   2 |   700/   14 batches | ms/batch 86.37 | loss  0.11354\n",
      "| epoch   2 |   800/   14 batches | ms/batch 86.37 | loss  0.10505\n",
      "| epoch   2 |   900/   14 batches | ms/batch 86.31 | loss  0.10602\n",
      "| epoch   2 |  1000/   14 batches | ms/batch 86.53 | loss  0.10911\n",
      "| epoch   2 |  1100/   14 batches | ms/batch 86.35 | loss  0.11516\n",
      "| epoch   2 |  1200/   14 batches | ms/batch 86.45 | loss  0.10775\n",
      "| epoch   2 |  1300/   14 batches | ms/batch 86.70 | loss  0.11609\n",
      "| epoch   2 |  1400/   14 batches | ms/batch 86.45 | loss  0.10992\n",
      "| epoch   2 |  1500/   14 batches | ms/batch 86.28 | loss  0.11137\n",
      "| epoch   2 |  1600/   14 batches | ms/batch 86.42 | loss  0.10117\n",
      "| epoch   2 |  1700/   14 batches | ms/batch 86.26 | loss  0.10946\n",
      "| epoch   2 |  1800/   14 batches | ms/batch 86.24 | loss  0.10535\n",
      "syn mse 0.207355222131452\n",
      "sen_mse 0.053729281466231384\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/   14 batches | ms/batch 87.26 | loss  0.21990\n",
      "| epoch   3 |   200/   14 batches | ms/batch 87.20 | loss  0.21879\n",
      "| epoch   3 |   300/   14 batches | ms/batch 86.14 | loss  0.22039\n",
      "| epoch   3 |   400/   14 batches | ms/batch 86.35 | loss  0.22009\n",
      "| epoch   3 |   500/   14 batches | ms/batch 86.04 | loss  0.22409\n",
      "| epoch   3 |   600/   14 batches | ms/batch 86.01 | loss  0.21133\n",
      "| epoch   3 |   700/   14 batches | ms/batch 86.16 | loss  0.22170\n",
      "| epoch   3 |   800/   14 batches | ms/batch 85.93 | loss  0.21143\n",
      "| epoch   3 |   900/   14 batches | ms/batch 86.07 | loss  0.21463\n",
      "| epoch   3 |  1000/   14 batches | ms/batch 86.06 | loss  0.21608\n",
      "| epoch   3 |  1100/   14 batches | ms/batch 86.24 | loss  0.22067\n",
      "| epoch   3 |  1200/   14 batches | ms/batch 86.08 | loss  0.22491\n",
      "| epoch   3 |  1300/   14 batches | ms/batch 86.16 | loss  0.21584\n",
      "| epoch   3 |  1400/   14 batches | ms/batch 86.13 | loss  0.20940\n",
      "| epoch   3 |  1500/   14 batches | ms/batch 86.19 | loss  0.20848\n",
      "| epoch   3 |  1600/   14 batches | ms/batch 86.20 | loss  0.21199\n",
      "| epoch   3 |  1700/   14 batches | ms/batch 86.24 | loss  0.21066\n",
      "| epoch   3 |  1800/   14 batches | ms/batch 86.12 | loss  0.21072\n",
      "| epoch   3 |   100/   14 batches | ms/batch 87.46 | loss  0.11388\n",
      "| epoch   3 |   200/   14 batches | ms/batch 86.29 | loss  0.11197\n",
      "| epoch   3 |   300/   14 batches | ms/batch 86.06 | loss  0.12452\n",
      "| epoch   3 |   400/   14 batches | ms/batch 86.44 | loss  0.11418\n",
      "| epoch   3 |   500/   14 batches | ms/batch 86.05 | loss  0.11350\n",
      "| epoch   3 |   600/   14 batches | ms/batch 86.15 | loss  0.11066\n",
      "| epoch   3 |   700/   14 batches | ms/batch 86.48 | loss  0.10728\n",
      "| epoch   3 |   800/   14 batches | ms/batch 86.09 | loss  0.11221\n",
      "| epoch   3 |   900/   14 batches | ms/batch 86.11 | loss  0.10717\n",
      "| epoch   3 |  1000/   14 batches | ms/batch 86.03 | loss  0.10867\n",
      "| epoch   3 |  1100/   14 batches | ms/batch 86.48 | loss  0.10323\n",
      "| epoch   3 |  1200/   14 batches | ms/batch 86.15 | loss  0.09925\n",
      "| epoch   3 |  1300/   14 batches | ms/batch 85.99 | loss  0.10266\n",
      "| epoch   3 |  1400/   14 batches | ms/batch 86.26 | loss  0.10957\n",
      "| epoch   3 |  1500/   14 batches | ms/batch 86.13 | loss  0.09791\n",
      "| epoch   3 |  1600/   14 batches | ms/batch 86.20 | loss  0.11723\n",
      "| epoch   3 |  1700/   14 batches | ms/batch 86.19 | loss  0.10102\n",
      "| epoch   3 |  1800/   14 batches | ms/batch 85.94 | loss  0.10584\n",
      "syn mse 0.20224378862951556\n",
      "sen_mse 0.05335074204664964\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/   14 batches | ms/batch 87.20 | loss  0.21396\n",
      "| epoch   4 |   200/   14 batches | ms/batch 86.15 | loss  0.21003\n",
      "| epoch   4 |   300/   14 batches | ms/batch 86.01 | loss  0.21343\n",
      "| epoch   4 |   400/   14 batches | ms/batch 86.20 | loss  0.21366\n",
      "| epoch   4 |   500/   14 batches | ms/batch 86.25 | loss  0.21280\n",
      "| epoch   4 |   600/   14 batches | ms/batch 86.15 | loss  0.21125\n",
      "| epoch   4 |   700/   14 batches | ms/batch 86.21 | loss  0.20668\n",
      "| epoch   4 |   800/   14 batches | ms/batch 86.18 | loss  0.21178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   900/   14 batches | ms/batch 85.88 | loss  0.20505\n",
      "| epoch   4 |  1000/   14 batches | ms/batch 86.00 | loss  0.22156\n",
      "| epoch   4 |  1100/   14 batches | ms/batch 86.25 | loss  0.20681\n",
      "| epoch   4 |  1200/   14 batches | ms/batch 86.03 | loss  0.20722\n",
      "| epoch   4 |  1300/   14 batches | ms/batch 86.10 | loss  0.20706\n",
      "| epoch   4 |  1400/   14 batches | ms/batch 86.31 | loss  0.21246\n",
      "| epoch   4 |  1500/   14 batches | ms/batch 86.14 | loss  0.20488\n",
      "| epoch   4 |  1600/   14 batches | ms/batch 86.39 | loss  0.21542\n",
      "| epoch   4 |  1700/   14 batches | ms/batch 86.40 | loss  0.20875\n",
      "| epoch   4 |  1800/   14 batches | ms/batch 85.93 | loss  0.21048\n",
      "| epoch   4 |   100/   14 batches | ms/batch 87.08 | loss  0.11194\n",
      "| epoch   4 |   200/   14 batches | ms/batch 86.01 | loss  0.10791\n",
      "| epoch   4 |   300/   14 batches | ms/batch 86.13 | loss  0.10390\n",
      "| epoch   4 |   400/   14 batches | ms/batch 85.99 | loss  0.10286\n",
      "| epoch   4 |   500/   14 batches | ms/batch 86.19 | loss  0.10724\n",
      "| epoch   4 |   600/   14 batches | ms/batch 86.19 | loss  0.10177\n",
      "| epoch   4 |   700/   14 batches | ms/batch 86.07 | loss  0.09958\n",
      "| epoch   4 |   800/   14 batches | ms/batch 86.15 | loss  0.10622\n",
      "| epoch   4 |   900/   14 batches | ms/batch 88.04 | loss  0.10917\n",
      "| epoch   4 |  1000/   14 batches | ms/batch 86.24 | loss  0.10762\n",
      "| epoch   4 |  1100/   14 batches | ms/batch 86.40 | loss  0.09582\n",
      "| epoch   4 |  1200/   14 batches | ms/batch 86.34 | loss  0.10881\n",
      "| epoch   4 |  1300/   14 batches | ms/batch 86.08 | loss  0.10043\n",
      "| epoch   4 |  1400/   14 batches | ms/batch 86.20 | loss  0.11000\n",
      "| epoch   4 |  1500/   14 batches | ms/batch 86.24 | loss  0.11135\n",
      "| epoch   4 |  1600/   14 batches | ms/batch 86.24 | loss  0.11629\n",
      "| epoch   4 |  1700/   14 batches | ms/batch 86.25 | loss  0.11596\n",
      "| epoch   4 |  1800/   14 batches | ms/batch 86.28 | loss  0.10194\n",
      "syn mse 0.19201647114549947\n",
      "sen_mse 0.05237012031750801\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/   14 batches | ms/batch 87.62 | loss  0.21102\n",
      "| epoch   5 |   200/   14 batches | ms/batch 86.02 | loss  0.20593\n",
      "| epoch   5 |   300/   14 batches | ms/batch 86.41 | loss  0.20803\n",
      "| epoch   5 |   400/   14 batches | ms/batch 86.19 | loss  0.20958\n",
      "| epoch   5 |   500/   14 batches | ms/batch 86.05 | loss  0.21249\n",
      "| epoch   5 |   600/   14 batches | ms/batch 86.35 | loss  0.20868\n",
      "| epoch   5 |   700/   14 batches | ms/batch 86.18 | loss  0.20417\n",
      "| epoch   5 |   800/   14 batches | ms/batch 86.66 | loss  0.21082\n",
      "| epoch   5 |   900/   14 batches | ms/batch 86.11 | loss  0.20466\n",
      "| epoch   5 |  1000/   14 batches | ms/batch 86.30 | loss  0.19774\n",
      "| epoch   5 |  1100/   14 batches | ms/batch 86.32 | loss  0.19422\n",
      "| epoch   5 |  1200/   14 batches | ms/batch 86.07 | loss  0.20420\n",
      "| epoch   5 |  1300/   14 batches | ms/batch 86.23 | loss  0.20829\n",
      "| epoch   5 |  1400/   14 batches | ms/batch 86.10 | loss  0.20172\n",
      "| epoch   5 |  1500/   14 batches | ms/batch 86.50 | loss  0.20938\n",
      "| epoch   5 |  1600/   14 batches | ms/batch 86.17 | loss  0.19374\n",
      "| epoch   5 |  1700/   14 batches | ms/batch 86.09 | loss  0.19522\n",
      "| epoch   5 |  1800/   14 batches | ms/batch 86.27 | loss  0.20469\n",
      "| epoch   5 |   100/   14 batches | ms/batch 87.22 | loss  0.10433\n",
      "| epoch   5 |   200/   14 batches | ms/batch 85.94 | loss  0.10392\n",
      "| epoch   5 |   300/   14 batches | ms/batch 86.34 | loss  0.10464\n",
      "| epoch   5 |   400/   14 batches | ms/batch 85.98 | loss  0.11323\n",
      "| epoch   5 |   500/   14 batches | ms/batch 86.23 | loss  0.10005\n",
      "| epoch   5 |   600/   14 batches | ms/batch 86.19 | loss  0.10687\n",
      "| epoch   5 |   700/   14 batches | ms/batch 85.96 | loss  0.11209\n",
      "| epoch   5 |   800/   14 batches | ms/batch 86.13 | loss  0.10963\n",
      "| epoch   5 |   900/   14 batches | ms/batch 86.06 | loss  0.10386\n",
      "| epoch   5 |  1000/   14 batches | ms/batch 86.50 | loss  0.11237\n",
      "| epoch   5 |  1100/   14 batches | ms/batch 86.06 | loss  0.11366\n",
      "| epoch   5 |  1200/   14 batches | ms/batch 86.14 | loss  0.10509\n",
      "| epoch   5 |  1300/   14 batches | ms/batch 86.33 | loss  0.10676\n",
      "| epoch   5 |  1400/   14 batches | ms/batch 86.31 | loss  0.10270\n",
      "| epoch   5 |  1500/   14 batches | ms/batch 86.29 | loss  0.10464\n",
      "| epoch   5 |  1600/   14 batches | ms/batch 86.21 | loss  0.10628\n",
      "| epoch   5 |  1700/   14 batches | ms/batch 86.44 | loss  0.10280\n",
      "| epoch   5 |  1800/   14 batches | ms/batch 85.99 | loss  0.10990\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        training(False, train_loader)\n",
    "        training(True, train_loader)\n",
    "        evaluate(test_loader)\n",
    "        print('-'*89)\n",
    "except KeyboardInterrupt:\n",
    "    print('-'*89)\n",
    "    print('Existing from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model save\n",
    "#torch.save(model.state_dict(), data_path+'exp4-id-feat-gene30-ALMANAC.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model load\n",
    "#model.load_state_dict(torch.load(data_path+'exp4-id-feat-gene30.p'))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the general model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_score, y_true, k):\n",
    "    \"\"\"\n",
    "        https://www.kaggle.com/davidgasquez/ndcg-scorer\n",
    "        y_true: np.array, size= [n_samples]\n",
    "        y_score: np.array, size=[n_samples]\n",
    "        k: int, rank\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    \n",
    "    #gain = 2 ** y_true -1\n",
    "    gain = y_true \n",
    "    \n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain/discounts)\n",
    "\n",
    "def evaluate_accuracy(data_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    syn_all=[]\n",
    "    syn_true_all=[]\n",
    "    ri1_all=[]\n",
    "    ri1_true_all=[]\n",
    "    ri2_all=[]\n",
    "    ri2_true_all=[]\n",
    "    \n",
    "    #loss\n",
    "    with torch.no_grad():\n",
    "        for iteration, sample in enumerate(data_loader):\n",
    "            d1=Variable(sample['d1'])\n",
    "            d1_fp = Variable(sample['d1_fp'].float())\n",
    "            d1_sm = Variable(sample['d1_sm'])\n",
    "            d1_gn = Variable(sample['d1_gn'].float())\n",
    "\n",
    "            d2=Variable(sample['d2'])\n",
    "            d2_fp = Variable(sample['d2_fp'].float())\n",
    "            d2_sm = Variable(sample['d2_sm'])\n",
    "            d2_gn = Variable(sample['d2_gn'].float())\n",
    "\n",
    "            cell = Variable(sample['cell'])\n",
    "            c_ts = Variable(sample['c_ts'])\n",
    "            c_ds = Variable(sample['c_ds'])\n",
    "            c_gn = Variable(sample['c_gn'].float())\n",
    "\n",
    "            syn_true = Variable(sample['syn'].float())\n",
    "            ri_d1=Variable(sample['ri_d1'])\n",
    "            ri_d2=Variable(sample['ri_d2'])\n",
    "\n",
    "\n",
    "            if cuda:\n",
    "                d1=d1.cuda()\n",
    "                d1_fp=d1_fp.cuda()\n",
    "                d1_sm=d1_sm.cuda()\n",
    "                d1_gn=d1_gn.cuda()\n",
    "\n",
    "                d2=d2.cuda()\n",
    "                d2_fp=d2_fp.cuda()\n",
    "                d2_sm=d2_sm.cuda()\n",
    "                d2_gn=d2_gn.cuda()\n",
    "\n",
    "                cell=cell.cuda()\n",
    "                c_ts=c_ts.cuda()\n",
    "                c_ds=c_ds.cuda()\n",
    "                c_gn=c_gn.cuda()\n",
    "\n",
    "\n",
    "            syn,ri1,ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts,c_ds,c_gn) )\n",
    "            \n",
    "            syn_all.append(syn.data.cpu().numpy())\n",
    "            syn_true_all.append(syn_true.numpy())\n",
    "            \n",
    "            ri1_all.append(ri1.data.cpu().numpy())\n",
    "            ri1_true_all.append(ri_d1.numpy())\n",
    "            \n",
    "            ri2_all.append(ri2.data.cpu().numpy())\n",
    "            ri2_true_all.append(ri_d2.numpy())\n",
    "            \n",
    "    return syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate synergy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all= evaluate_accuracy(test_loader)\n",
    "\n",
    "syn_all= [s.item() for syn in syn_all for s in syn]\n",
    "syn_true_all = [s for syn in syn_true_all for s in syn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG\n",
    "dcg_score(syn_all,syn_true_all, k=20)/dcg_score(syn_true_all,syn_true_all, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUPRC\n",
    "metrics.average_precision_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUROC\n",
    "metrics.roc_auc_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate sensitivity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri1_all= [r.item() for ri in ri1_all for r in ri]\n",
    "ri1_true_all = [r for ri in ri1_true_all for r in ri]\n",
    "\n",
    "ri2_all= [r.item() for ri in ri2_all for r in ri]\n",
    "ri2_true_all = [r for ri in ri2_true_all for r in ri]\n",
    "\n",
    "ri_all=ri1_all+ri2_all\n",
    "ri_true_all=ri1_true_all+ri2_true_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG\n",
    "dcg_score(ri_all,ri_true_all, k=20)/dcg_score(ri_true_all,ri_true_all, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "metrics.roc_auc_score(ri_true_all,  1/(1 + np.exp(-np.array(ri_all))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer the general model to specific model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to boost a bit more with general model's test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use major's test set\n",
    "training(False, test_loader)\n",
    "training(True, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the layer's ID that we'd like to fix or free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(model.parameters()):\n",
    "    print(i, param.size(), param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_after = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(model.parameters()):\n",
    "    if i>=release_after:\n",
    "        param.requires_grad=True\n",
    "    else:\n",
    "        param.requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prostate or bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_loader_minor=_train_loader_prostate\n",
    "_test_loader_minor=_test_loader_prostate\n",
    "_test_minor=_test_prostate\n",
    "#_train_loader_minor=_train_loader_bone\n",
    "#_test_loader_minor=_test_loader_bone\n",
    "#_test_minor=_test_bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use minor's train set\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        training(False, _train_loader_minor)\n",
    "        training(True, _train_loader_minor)\n",
    "        evaluate(_test_loader_minor)\n",
    "        print('-'*89)\n",
    "except KeyboardInterrupt:\n",
    "    print('-'*89)\n",
    "    print('Existing from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate synergy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all= evaluate_accuracy(_test_loader_minor)\n",
    "\n",
    "syn_all= [s.item() for syn in syn_all for s in syn]\n",
    "syn_true_all = [s for syn in syn_true_all for s in syn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG\n",
    "dcg_score(syn_all,syn_true_all, k=20)/dcg_score(syn_true_all,syn_true_all, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUROC\n",
    "metrics.roc_auc_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUPRC\n",
    "metrics.average_precision_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate sensitivity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri1_all= [r.item() for ri in ri1_all for r in ri]\n",
    "ri1_true_all = [r for ri in ri1_true_all for r in ri]\n",
    "\n",
    "ri2_all= [r.item() for ri in ri2_all for r in ri]\n",
    "ri2_true_all = [r for ri in ri2_true_all for r in ri]\n",
    "\n",
    "ri_all=ri1_all+ri2_all\n",
    "ri_true_all=ri1_true_all+ri2_true_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG\n",
    "dcg_score(ri_all,ri_true_all, k=20)/dcg_score(ri_true_all,ri_true_all, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "metrics.roc_auc_score(ri_true_all,  1/(1 + np.exp(-np.array(ri_all))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the top ranked drug combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_all_prob=1/(1 + np.exp(-np.array(syn_all)))\n",
    "order = np.argsort(syn_all_prob)[::-1]\n",
    "syn_true_all_order = np.take(syn_true_all, order[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    comb=_test_minor[order[k]]\n",
    "    print(codes['drugs'].idx2item[comb['d1']], ',', \n",
    "          codes['drugs'].idx2item[comb['d2']], ',', \n",
    "          codes['cell'].idx2item[comb['cell']],  ',', \n",
    "          codes['tissue'].idx2item[comb['c_ts']],  ',',\n",
    "          codes['disease'].idx2item[comb['c_ds']], ',', \n",
    "          comb['ri_d1'], ',',\n",
    "          comb['ri_d2'], ',',\n",
    "          syn_true_all_order[k])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
